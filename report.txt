 	In simulating my cache, what I used was multi-layered arrays formed from structs. For instance, the at the top layer would have a struct representing a cache, then a pointer to another struct representing sets, and finally a pointer to the last struct representing the number of lines. This method is highly efficient in look-up time since array access is O(1) given the input data. For example, in this assignment, set indices were given which makes looking up lines much faster. As opposed to making a linked list, which would undoubtedly take much more time. To simulate the memory accesses required getting a tag ID and a set ID from each address, resulting in a cache hit if the identified cache block is found. If the tag bits and set bits from an address is nowhere to be found in the cache set, the cache is then loaded with that block. Prefetching works in a similar manner, except the next address's tag bits and set bits are used to check if it's not in the cache. Then, for the set-associative and n-way mapping, I used a FIFO algorithm through array shifting. The lines in the set would be shifted down by 1, and the last line will have the added data. I realize that this will be slow and expensive, but it is simple in nature. I determine that the method of simulating a cache is correct because it simulated a large amount of cache hits than cache misses and prefetching made it even better.

	The prefetcher affects the number of cache hits and memory reads by grabbing the next cache block before the processor requests the item. The reason why this is a positive thing is because of the boost in execution performance from grabbing in the small and fast cache, rather than the slow and large memory. For instance, if a block A+B is loaded into the cache after the address A is loaded in, when the next address is read, block A+B would have already been there. This would increase the number of cache hits because when searching through the cache set, the block would already be there, which would result in a cache hit. Of course, the memory reads would also increase due to reading in the prefetched block after reading the non-prefetched block. This is consistent throughout all testing because grabbing from a cache is much more faster than grabbing from memory. However, this was not consistent across all testing. For instance, for the n-way associative set, the closer the associativity is to the block size, the less effective prefetching is. In fact, when they were equal to each other, prefetching worked even worse than without prefetching.  	  

	It is always harder to make large memories run faster. Therefore, larger caches usually increase the cache hit-time, and this is why L1 cache is smaller than an L2 cache. To simulate an L2 cache would require a form of communication between L1 cache and L2 cache. In my simulation, the L1 cache only had a form of communication with the main memory. In order to simulate both caches, the L1 cache would need to be able to access elements stored in the L2 cache. As a result, I would have my L1 cache have separate data and instructions whereas the L2 cache would be unified. I would make the L2 cache be unified because of the flexible use in memory, which would result in a larger hit rate. In my program, I would have to create another bigger cache from the cache struct I mentioned before. Then, I would need to make sure the capacity sizes for both caches do not exceed what is given. Also, I will make the L1 cache a write-through cache, and the L2- a write back cache. This is the best type of connection because write-through (L1) will immediately seek the recipient (L2) and L2 would defer from main memory as much as possible, resulting in a great hit-time.        

	In a write-through cache, the cache would immediately write data back to memory. However, if I simulate a write-back cache, it would be able to delay writing blocks back to memory for as long as possible. To simulate a write-back cache would require some sort of modification state. This will make sure that the cache must write back into the cache line before the line gets evicted, in preparation for replacement. If the block has already been written to, and we need to evict it, we would need to write the cached copy into a lower-level. To find out whether the block has been written to or not, the dirty bit would be used as a signal for a modified state. Then, the dirty bit is set when it is written to a block. Finally, to get the block out, we would need to write to lower-level. This aligns with the principle of write-allocate: write-back caches are usually write-allocate in order to load the block into memory and then update.  
